---
title: "Regresión"
output: html_notebook
---

# Regresión lineal

## Regresión lineal una variable independiente

La regresión lineal trata de encontrar el valor óptimo de diferentes coeficientes $\beta_x$ una ecuación de tipo.


$$
y_i=\beta_0+\beta_1 x_i+\varepsilon_i  \quad \text{para }i=1,\dots,n
$$


donde:

* $y_i$: i-esimo valor de la variable dependiente
* $x_i$: i-esimo valor de la variable independiente
* $\beta_0, \beta_1$: parámetros a determinal que dado un conjunto de $x_i$ produce los mejores $y_i$
 * $\beta_0$ : Puede ser llamado sesgo, bias, intercept o término constante. Indica el corte en el eje Y
 * $\beta_1$ : Puede ser llamado pendiente, slope. Indica cuanto aumenta Y por cada incremento de X
* $\varepsilon_i$: error, valor aleatorio.

de tal forma que nuestra predicción de $y_i$ será:
$$
\hat{y_i}=\beta_0+\beta_1 x_i  \quad \text{para }i=1,\dots,n
$$


Estos coeficientes de $\beta_x$ son aquellos garantizan un valor mínimo de la suma del error cuadrático:

$$
MSE = {1 \over n} \sum_{i=0}^n{(y_i-\hat{y_i})^2}
$$


Vamos a simular un dataset de datos. Unos puntos sobre los ejes X e Y que guardan la siguiente relación:

\begin{equation}
y=3*x+5
\end{equation}

pero a los cuales le añadimos ruido, que bien podría simular errores en la medida, este ruido será gausiano y tendrá una media 0 y desviación típica de 10.


```{r}
library(ggplot2)
set.seed(123)
n<-100
x<-seq(-1,10,length.out = n)
df_ejemplo_reg_lin<-data.frame(x,y = 3*x+5+rnorm(n,mean=0,sd=10))
ggplot(df_ejemplo_reg_lin, aes(x,y))+geom_point()
```


Ahora vamos a crear el modelo que encuentre la relación entre X e Y:

```{r}
model <- lm(y~x,data=df_ejemplo_reg_lin)
summary(model)
```

Esto nos indica que la relación entre X e Y que ha encontrado el modelo se podría resumir en la siguiente ecuación:
\begin{equation}
\hat{y} = 4.8871 + 3.226*x
\end{equation}

Además del coeficiente nos indica la desviación típica del error, además nos indica el estimador estadístico t (sacado de una distribución t-student) y el pvalor del coeficiente. Este pvalor refleja la probabilidad de que por puro azar los puntos se hayan distribuido de tal forma que su coeficiente sea diferente de 0. Así parte de dos hipótesis:


$$
H_0 : \beta_i = 0 \\
H_1 : \beta_i \ne 0
$$

Los * al lado del pvalor nos indica el grado de confianza en el coeficiente. Valores por debajo de 0.05 indican una buena calidad del coeficiente.

Los intevalos de confianza de los coeficientes se pueden calcular con confint():
```{r}
confint(model, level=0.95)
```



Si nos llega un nuevo valor de X podremos calcular el valor de Y utilizando la función predict(). Fijaros que a la función predict() tenemos que pasarle un dataframe con las mismas columnas con las que entrenamos el modelo:
```{r}
nuevo_valor<-data.frame(x=4)
y_pred <- predict(model, nuevo_valor)
paste0("El valor predicho de y(",nuevo_valor$x,") es:",y_pred)
```
Esto equivale a usar la fórmula:

\begin{equation}
\hat{y} = 4.8871 + 3.226*4 = 17.7911
\end{equation}

También podemos obtener el intevalo de confianza con las opciones interval:

* **Opción interval = 'confidence'**: Un **intervalo de confianza** de la predicción es un rango que probablemente contiene el **valor medio de la variable dependiente** dados los valores específicos de las variables independientes. Estos intervalos proporcionan un rango para el promedio de la población. Estos rangos no dicen nada sobre la distribución de los puntos de datos individuales alrededor de la media de la población.
```{r}
predict(model, nuevo_valor, interval = 'confidence')
```

* **Opción interval = 'prediction'**: Un **intervalo de predicción** es un rango que probablemente contiene el **valor de la variable dependiente** para una sola observación nueva dados los valores específicos de las variables independientes. Con este tipo de intervalo, estamos prediciendo rangos para observaciones individuales en lugar del valor medio.
```{r}
predict(model, nuevo_valor, interval = 'prediction')
```



Para el dataframe oridinal de datos vamos a calcular y pintar como sería nuestra predicción:

```{r}
library(dplyr)

df_ejemplo_reg_lin_pred_conf_conf <- data.frame(
  predict(model, df_ejemplo_reg_lin, interval='confidence', level=0.9)) %>% 
  rename_all( ~ paste0("conf_", .x))

df_ejemplo_reg_lin_pred_conf_pred<-data.frame(
  predict(model, df_ejemplo_reg_lin, interval='prediction', level=0.9)) %>% 
  rename_all( ~ paste0("pred_", .x))

df_pred = cbind(df_ejemplo_reg_lin_pred_conf_conf,
                df_ejemplo_reg_lin_pred_conf_pred,
                df_ejemplo_reg_lin)

ggplot(df_pred,aes(x=x))+geom_point(aes(y=y))+
  geom_line(aes(y=conf_fit,color='Media'))+
  geom_line(aes(y=conf_upr,color='confidence'))+
  geom_line(aes(y=conf_lwr,color='confidence'))+
  geom_line(aes(y=pred_upr,color='prediction'))+
  geom_line(aes(y=pred_lwr,color='prediction'))+
  scale_color_discrete(name = "Predicciones", labels = c("confidence","media","prediction"))+theme_bw()

```



## Regresión lineal múltiple

Es un modelo matemático usado para aproximar la relación de dependencia entre una variable dependiente $y$, las variables independientes $x_i$ y un término aleatorio $\varepsilon$. Este modelo puede ser expresado como:

$$
y_i=\beta_0 + \beta_1 · x_{i1}+\beta_2 · x_{i2}+\cdots +\beta_p · x_{ip}+\varepsilon_{i} = \sum \beta_k · x_{ik}+\varepsilon_{i}
$$


donde:
* $y_i$: variable dependiente
* $x_{i1},x_{i2},\cdots ,x_{ip}$: variables independientes
* $\beta_0, \beta_1,\beta_2,\cdots ,\beta_p$: parámetros a determinal que dado un $x_i$ produce el mejor posible $y_i$
* $\varepsilon_i$: error, valor aleatorio.

Condiciones para aplicarlo:
1. Que la relación entre las variables sea lineal.
2. Que los errores en la medición de $x_{ik}$ sean independientes entre sí.
3. Que los errores tengan varianza constante. (https://es.wikipedia.org/wiki/Heterocedasticidad)
4. Que los errores tengan una media aritmética igual a cero.


Vamos a simular un dataset de datos. Unos puntos sobre los ejes X1,X2 e Y que guardan la siguiente relación:

\begin{equation}
y=3*x1 + 4*x2^2 + 5
\end{equation}

```{r}
set.seed(123)
n<-500
x1<-runif(n=n, min=-10, max=10)
x2<-runif(n=n, min=-10, max=10)
df_ejemplo_reg_mult<-data.frame(x1, x2,y = 3*x1+4*x2^2+5+rnorm(n,mean=0,sd=10))
ggplot(data=df_ejemplo_reg_mult,aes(x=x1,y=x2,color=y))+geom_point()
```

Para ver los puntos en 3d mejor usamos la libreria plotly
```{r}
library(plotly)
plot_ly(data=df_ejemplo_reg_mult, x=~x1, y=~x2, z=~y, size=1, type='scatter3d', mode='markers' )
```


Ahora vamos a crear el modelo que encuentre la relación entre X1, X2 e Y:

```{r}
model <- lm(y~x1+I(x2^2),data=df_ejemplo_reg_mult)
summary(model)
```


Esto nos indica que la relación entre X1, X2 e Y que ha encontrado el modelo se podría resumir en la siguiente ecuación:
\begin{equation}
\hat{y} = 4.38890 + 2.93336*x1 + 4.01723*{x2}^2
\end{equation}

Podemos convertir el objeto del modelo en un dataframe:
```{r}
 broom::tidy(model)
```


Igual que antes tenemos información de la desviación típica del error y de su pvalor.
Los intevalos de confianza de los coeficientes se pueden calcular con confint():
```{r}
confint(model, level=0.95)
```

Si nos llega un nuevo valor de X1 y X2 podremos calcular el valor de Y utilizando la función predict():
```{r}
nuevo_valor<-data.frame(x1=4,x2=3)
y_pred <- predict(model, nuevo_valor)
paste0("El valor predicho de y(",nuevo_valor$x,") es:",y_pred)
```

Esto equivale a usar la fórmula:

\begin{equation}
\hat{y} = 4.38890 + 2.93336*4 + 4.01723*3^2=52.27741
\end{equation}

También podemos obtener el intevalo de confianza con las opciones interval:

* **Opción interval = 'confidence'**:

```{r}
predict(model, nuevo_valor, interval = 'confidence')
```

* **Opción interval = 'prediction'**:
```{r}
predict(model, nuevo_valor, interval = 'prediction')
```

Para el dataframe oridinal de datos vamos a calcular y pintar como sería nuestra predicción:

```{r}
df_pred <- df_ejemplo_reg_mult
df_pred$pred <- predict(model, df_ejemplo_reg_mult)
df_pred %>% plot_ly(x=~x1, y=~x2, z=~y, size=1, type='scatter3d', mode='markers', name='real') %>% 
  add_trace(z = ~pred, name="predicho") 
```



```{r}
x1_grid<-seq(-10,10,length.out=10)
x2_grid<-seq(-10,10,length.out=10)
plane <- outer(x1_grid, x2_grid, function(x2, x1) predict(model,data.frame(x1,x2)) )

df_ejemplo_reg_mult %>% plot_ly(x=~x1, y=~x2, z=~y, size=1, type='scatter3d', mode='markers', name='real') %>% 
  add_surface(x=~x1_grid, y=~x2_grid, z=~plane, showscale = FALSE) 
```


## Formulas en R

En R muchas funciones como la regresión lineal que hemos visto esperan recibir una fórmula para tratar de entender cual es la relación entre diferentes variables. Hasta ahora hemos visto dos ejemplos sencillos. El primero era el de la regresión lineal simple. Para decirle a R que la variable `y` sigue la formula $y=\beta_0+\beta_1·x$ se lo describiamos como `y ~ x`. Para decirle a R que la variable `y` sigue la formula $y=\beta_0+\beta_1·x+\beta_2·t$ se lo diríamos como `y~x+t`

Las formulas son objetos en R y se pueden grabar como cualquier otro objeto:
```{r}
g <- y ~ x
class(g)
```
Y a su vez este objeto se puede pasar a una función como lm() en el parámetro fórmula. Tal y como hemos visto:
```{r}
lm(df_ejemplo_reg_lin, formula=g)
```



Pero además del simbolo `+` existen más símbolos que permiten distintas formas de interacción entre variables. 

* `-` : Sirve para eliminar términos
* `.` : Abreviatura para todas las columnas
* `*` : Multiplicación ej: `z~x*y` es equivalente a `z=\beta_0+beta_1·x+beta_2·y+beta_3·x·y`
* `:` : Cruce de dos variable ej: `z~x:y` es equivalente a `z=\beta_0+beta_1·x·y`
* `I()`: Permite elevar a la potencia la variable seleccionada. ej: `I(x^2)`
* `+0` : Elimina el sesgo (intercept). ej: `y~x+0`equivale a `y = \beta_0·x`

Por ejemplo:
```{r}
lm(df_ejemplo_reg_mult, formula=y~x1 + abs(x2) + 0 )
```
Nos buscará los valores de $\beta_i$ que encajen mejor para la fórmula  $y=\beta_0·x1+\beta_2·abs(x2)$. El resultado es equivalente a la ecuación:

\begin{equation}
y = 3.198·x1+31.03·|x2|
\end{equation}

O el modelo:
```{r}
lm(df_ejemplo_reg_mult, formula=y~x1 * x2  )
```

Equivale a la ecuación:
\begin{equation}
y = 139.7560+3.1312·x1-1.7626·x2+0.1775·x1·x2
\end{equation}



#### Impresión de formula en R

Con el paquete *equatiomatic* puedes convertir un modelo en una fómula latex:
```{r}
library(equatiomatic)
model<-lm(df_ejemplo_reg_mult, formula=y~x1 * x2  )
extract_eq(model, use_coefs=T)
```




### Ejemplo:

Vamos a calcular la relación que existe entre el índice de bienestar (SWB) y el índice de desarrollo sostenible (SGD.index)

Primero nos bajamos los datos:
```{r}
library(readxl)
library(RCurl)
library(countrycode)
url_whr<-'https://happiness-report.s3.amazonaws.com/2020/WHR20_DataForFigure2.1.xls'
filename_whr <- '/tmp/WHR.xls'
download.file(url_whr,filename_whr)
df_whr <- read_excel(filename_whr)
df_whr <- df_whr[,1:3]
colnames(df_whr)<-c("pais","region","SWB")
df_whr$region <- factor(df_whr$region)

url_glir<-'https://raw.githubusercontent.com/sdsna/2019GlobalIndex/master/2019GlobalIndexResults.xlsx'
filename_glir <- '/tmp/GlobalIndex.xlsx'
download.file(url_glir,filename_glir)
df_glir <- read_excel(filename_glir,sheet=4,skip = 1)
df_glir <- df_glir[,1:3]
colnames(df_glir)<-c("pais","pais.id","SDG.index")

df_whr$pais.id<-countrycode(df_whr$pais, 'country.name', 'iso3c')
df_total <- merge(df_glir, df_whr, by='pais.id', suffixes = c('.glir','.whr'))
df_total$bandera<-countrycode(df_total$pais.id, 'iso3c', 'unicode.symbol')

```

A continuación probamos con un modelo. Con la fase exploratoria del notebook 3 habíamos visto que sospechabamos de una función de orden 2:
```{r}
model<-lm(SWB~SDG.index+I(SDG.index^2), data=df_total)
summary(model)
```

El intervalo de confianza de los coeficientes es:
```{r}
confint(model)
```


Vamos a analizar los residuos a ver si hay muchos outliers ya si tiene una distribución normal:
```{r}
hist(model$residuals, breaks=10)
```



Vamos a mirar los residuos, aquellos paises que tienen un índice de felicidad mucho mayor que el que podríamos esperar por su índice de desarrollo sostenible:

```{r}
df_total$pred <- predict(model,df_total)
df_total$residual<-df_total$SWB-df_total$pred
df_total<-df_total[order(df_total$residual, decreasing = T),
                   c("pais.glir","SDG.index","SWB","residual","bandera","pred")]
head(df_total)
```


Otra forma de ordenar, descenciente:
```{r}
df_total %>% arrange(desc(residual))
```

```{r}
ggplot(df_total,aes(x=SDG.index, y=residual))+geom_text(aes(label=bandera))
```



Podemos pintar la proyección y ver que coincide con la que hacía ggplot con geom_smooth:
```{r}
ggplot(df_total,aes(x=SDG.index, y=SWB))+
  geom_text(aes(label=bandera))+
  geom_line(aes(y=pred))+
  geom_smooth(method='lm',formula=y~x+I(x^2))
```



## Regularización

Si el modelo es demasiado complejo ocurre el **sobreajuste** el modelo aprende sobre el ruido de nuestro modelo de entrenamiento y no es capaz de  generalizar bien.
Para evitar el sobreajuste(overfitting) se puede recurrir a simplificar el modelo o a poner limitaciones sobre el mismo. Esto se conoce con el nombre de regularización.

* Regularización Lasso o $L$: permite seleccionar los parámetros que más afectan al resultado. Se añade la función de coste:

$$
Coste = {1 \over n} \sum_{i=0}^n{(Y-\hat{Y})^2}+\lambda \sum_j | \beta |
$$

* Regularización Ridge o $L^2$: se evita que los parámetros crezcan demasiado. Se añade la función de coste:

$$
Coste = {1 \over n} \sum_{i=0}^n{(Y-\hat{Y})^2}+\lambda \sum_j \beta^2
$$

* Elástica: Una solución de compromiso entre las dos:

$$
Coste = {1 \over n} \sum_{i=0}^n{(Y-\hat{Y})^2}+ \alpha \lambda \sum_j | \beta |+(1-\alpha)/2 \lambda \sum_j \beta^2
$$

Para la regularización podemos usar el paquete [glmnet](https://glmnet.stanford.edu/articles/glmnet.html)

```{r}
n  <- 100
x1 <- runif(n,min=-10, max=10)
x2 <- rnorm(n,mean=0,sd=9)
x3 <- rnorm(n,mean=0,sd=2)

df_regularization  <- data.frame(x1,x2,x3)
df_regularization$y<- 3+0.1*x1+x2*2+x3*4+x1*x2*0.01+x3*x2*3 +rnorm(n,mean=0,sd=10)
```

Para aplicar la regularización tendríamos que ejecutar la función glmnet(). En este caso le vamos a pasar el valor de $\lambda=2$ y $\alpha=0.5$ que hemos decidido:

```{r}
library(glmnet)
X <- df_regularization %>% 
  select(x1,x2,x3) %>% 
    mutate(x1x2=x1*x2,
           x1x3=x1*x3,
           x2x3=x2*x3) %>% 
    as.matrix()
Y <- as.matrix(df_regularization[,'y'])
model<-glmnet(X,Y,lambda=2,alpha=0.5, standardize=T)
coefficients(model)
```


esta librería nos permite obtener el valor óptimo de $\lambda$ para un $\alpha$ dado. Si no se le indica lo contrario por óptimo entenderá la solución que genere un menor error cuadrático medio. Este proceso lo realiza utilizando validación cruzada (cross-validation):
```{r}
set.seed(123)
cvfit_model<-cv.glmnet(X,Y,nfolds=10,alpha=0.5, standarize=T)
paste("El mínimo RMSE se produce para lambda=",cvfit_model$lambda.min)
print("Los coeficientes para ese valor son:")
coef(cvfit_model, s = cvfit_model$lambda.min)
plot(cvfit_model)
``` 
Para predecir sobre valores nuevos tenemos primero que generar la nueva matriz X y posteriormente pasarsela al modelo para genere el valor de Y estimado:

```{r}
df_regularization_new <- data.frame(x1=2, x2=2.5, x3=0.5)
X_new <- df_regularization_new %>% 
  select(x1,x2,x3) %>% 
    mutate(x1x2=x1*x2,
      x1x3=x1*x3,
      x2x3=x2*x3) %>% as.matrix()

predict(cvfit_model,newx=X_new,s=cvfit_model$lambda.min)
```

Como muchas veces es más conveniente trabajar con fórmulas, existe una librería llamada glmnetUtils que es equivalente a glmnet pero funciona con dataframes y formulas en lugar de con matrices.
```{r}
model<-glmnetUtils::glmnet(formula=y~x1*x2+x1*x3+x2*x3, 
                           data=df_regularization, 
                           lambda=2,alpha=0.5, standardize=T)
coefficients(model)
```
Las mismas funciones que tenemos en el paquete glmnet lo están en glmnetUtils. Así podemos calcular el valor de $\lambda$ óptimo mediante validación cruzada:
```{r}
set.seed(123)
cvfit_model<-glmnetUtils::cv.glmnet(formula=y~x1*x2+x1*x3+x2*x3, data=df_regularization,nfolds=10,alpha=0.5, standarize=T)
paste("El mínimo RMSE se produce para lambda=",cvfit_model$lambda.min)
print("Los coeficientes para ese valor son:")
coef(cvfit_model, s = cvfit_model$lambda.min)
``` 
Para predecir sobre valores nuevos solo tenemos que generar el dataframe, la función se encarga de aplicar la fórmula:

```{r}
df_regularization_new <- data.frame(x1=2, x2=2.5, x3=0.5)

predict(cvfit_model,
        newdata=df_regularization_new,
        s=cvfit_model$lambda.min)
```



# Ejemplos


### Ejemplo toxicidad 

Extraido del artículo:
https://www.tandfonline.com/doi/abs/10.1080/1062936X.2015.1018938

Donde se trata de crear un modelo matemático de toxicidad (QSAR) que afecta a los peces Pimephales promelas basandose en 950 compuestos químicos con las siguientes características:

* CIC0: Complementary Information Content index (simetría de vecinos de orden-0). Es inversamente proporcional la cantidad de elementos químicos distintos que forman una molécula.
* SM1_Dz(Z): Momento espectral de orden 1 de la matriz 2D Barysz ajustada por su peso atómico. 
* GATS1i: Autocorrelación con retraso de 1 teniendo en cuenta el potencial de ionización. Tiende a tener valores bajos para moleculas con varios enlaces entre átomos de carbono como los compuestos aromáticos.
* NdsCH: Número de atomos de tipo dsCH  $=CH<$
* NdsCH: Número de atomos de tipo dssC  $=CH-$
* MLOGP: Coeficiente de partición octanol-agua de Moriguchi. Mide la solubilidad de una sustancia en octanol y agua.
* LC_50: Es la concentracion de una sustancia tóxica necesaria para matar al 50% de una población al cabo de 96 horas en  [-LOG(mol/L)]

El dataset lo descargamos de:
https://archive.ics.uci.edu/ml/datasets/QSAR+fish+toxicity

```{r}
df_toxicidad=read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00504/qsar_fish_toxicity.csv',
head=F, sep=';', 
col.names = c('cic0','sm1_dz','gats1l','ndsch','ndssc','mlogp','lc50'))
head(df_toxicidad)
```

Vamos a hacer un análisis exploratorio rápido:
```{r}
summary(df_toxicidad)
```


Observamos la correlación entre variables:
```{r}
cr_toxicidad<-cor(df_toxicidad)
cr_toxicidad
```

```{r}
library(corrplot)
corrplot(cr_toxicidad, type="upper", order="hclust",
         col=RColorBrewer::brewer.pal(n=8, name="RdYlBu"))
```

```{r}
library(GGally)
ggpairs(df_toxicidad, 
        progress=FALSE,
       lower = list(continuous = wrap("points", alpha = 0.3,size=0.1,color='blue')),
       upper = list(continuous = GGally::wrap("cor", family="sans")	)
       )
```

Queremos hacer un modelo y evaluar la calidad de su predicción. Para ello dividiremos nuestro dataset entre test y train:

```{r}
set.seed(42)
idx <- sample(1:nrow(df_toxicidad),nrow(df_toxicidad)*0.7)
train_toxicidad <- df_toxicidad[idx,]
test_toxicidad  <- df_toxicidad[-idx,]
```

Calculamos el modelo utilizando todas las variables:
```{r}
model_qsar <- lm(lc50~.-ndssc, train_toxicidad)
summary(model_qsar)
```

```{r}
hist(model_qsar$residual)
```
```{r}
qqnorm(model_qsar$residual)
qqline(model_qsar$residual,col='orange')
```

```{r}
test_toxicidad$pred <- predict(model_qsar , test_toxicidad)
caret::postResample(test_toxicidad$pred, test_toxicidad$lc50)
```



#### Modelo complejo

Podemos usar un modelo más complejo con interacciones entre variables:

```{r}
model_qsar <- lm(lc50~ cic0*sm1_dz*gats1l*ndsch*ndssc*mlogp, train_toxicidad)
summary(model_qsar)
```
Pero este modelo sufre de overfitting:
```{r}
test_toxicidad$pred <- predict(model_qsar , test_toxicidad)
caret::postResample(test_toxicidad$pred, test_toxicidad$lc50)
```


Podemos usar regularización para tratar de reducir el overfitting en este modelo:
```{r}
set.seed(123)
cvfit_qsar_model<-glmnetUtils::cv.glmnet(
  formula=lc50~ cic0*sm1_dz*gats1l*ndsch*ndssc*mlogp, 
  data=train_toxicidad, 
  alpha=.5,
  standarize=T)
paste("El mínimo RMSE se produce para lambda=",cvfit_qsar_model$lambda.min)
#coef(cvfit_qsar_model, s = cvfit_model$lambda.min)
``` 
Para predecir sobre valores nuevos solo tenemos que generar el dataframe, la función se encarga de aplicar la fórmula:

```{r}
test_toxicidad$pred <- predict(cvfit_qsar_model,
                               newdata=test_toxicidad,
                               s=cvfit_qsar_model$lambda.min) 
caret::postResample(test_toxicidad$pred, test_toxicidad$lc50)
```

## RandomForest

Podemos utilizar la librería Random Forest:

```{r}
library(randomForest)
rf_qsar <- randomForest(lc50~., data=train_toxicidad, 
                        importance=TRUE, maxnodes=30)
rf_qsar
```
```{r}
train_pred <- predict(rf_qsar,train_toxicidad) 
caret::postResample(train_pred, train_toxicidad$lc50)
```



```{r}
test_toxicidad$pred <- predict(rf_qsar,test_toxicidad) 
caret::postResample(test_toxicidad$pred, test_toxicidad$lc50)
```





## XGBoost

También podemos usar la librería XGBoost: https://xgboost.readthedocs.io/


```{r}
library(xgboost)
xgb_qsar <- xgboost(data=as.matrix(train_toxicidad %>% select(-lc50)), 
                    label=train_toxicidad$lc50,
                    params=list(max_depth=5),
                    nrounds=1000,
                    print_every_n=100,
                    verbose = 0)
xgb_qsar
```
Comparamos las figuras de training y test para ver si hay overfitting:
```{r}
train_pred<- predict(xgb_qsar,
  as.matrix(train_toxicidad[,c("cic0","sm1_dz","gats1l","ndsch","ndssc","mlogp")]))
print("Training:")
caret::postResample(train_pred, train_toxicidad$lc50)

test_toxicidad$pred <- predict(xgb_qsar,
  as.matrix(test_toxicidad[,c("cic0","sm1_dz","gats1l","ndsch","ndssc","mlogp")]))
print("Testing:")
caret::postResample(test_toxicidad$pred, test_toxicidad$lc50)
```
max_depth=3, min_child_weight=20, gamma=2


Vamos a buscar, mediante un grid search sobre diferentes parámetros cuales pueden ser los mejores para nuestro caso.
Los parámetros se pueden consultar en: https://xgboost.readthedocs.io/en/latest/parameter.html

```{r}
searchGrid <- expand.grid(subsample = c(0.8, 1), 
                                max_depth = c(3,4,5),
                                min_child_weight = c(5, 10),
                                gamma=c(2,3)
                                )


rmseErrors <- apply(searchGrid, 1, function(parameterList){
      xgboostModelCV <- xgb.cv(data=as.matrix(train_toxicidad %>% select(-lc50)), 
                               label=train_toxicidad$lc50,
                               nrounds = 100, nfold = 5, 
                               metrics = "rmse", verbose = FALSE, 
                               "eval_metric" = "rmse",
                         params=as.list(parameterList))
      
      rmse <- tail(xgboostModelCV$evaluation_log$test_rmse_mean,1)
      return(rmse)
    })
print("Los parámetros con menor RMSE son:")
searchGrid[which.min(rmseErrors),]
```


```{r}
parameterList<-as.list(searchGrid[which.min(rmseErrors),])
xgb_qsar <- xgboost(data=as.matrix(train_toxicidad %>% select(-lc50)), 
                    label=train_toxicidad$lc50,
                    params=parameterList,
                    nrounds=100,
                    verbose = 0)
xgb_qsar
```
```{r}
train_pred<- predict(xgb_qsar,
  as.matrix(train_toxicidad[,c("cic0","sm1_dz","gats1l","ndsch","ndssc","mlogp")]))
caret::postResample(train_pred, train_toxicidad$lc50)
```


```{r}
test_toxicidad$pred <- predict(xgb_qsar,
  as.matrix(test_toxicidad[,c("cic0","sm1_dz","gats1l","ndsch","ndssc","mlogp")]))
caret::postResample(test_toxicidad$pred, test_toxicidad$lc50)
```




