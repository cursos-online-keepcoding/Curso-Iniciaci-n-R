---
title: "Clustering"
output: html_notebook
---

R tiene diferentes librerías y funciones que permiten hacer clustering.
Vamos a ver dos muy populares, kmeans y clustering jerárquicos.

# K-means

Vamos a partir del dataset de diferentes aceites del paquete dslabs

Composición en porcentaje de ocho ácidos grasos encontrados en 572 aceites de oliva italianos:
* region. Región de Italia.
* area. Área de Italy.
* palmitic. % de ácido palmítico en la muestra.
* palmitoleic. % de ácido palmitoleico en la muestra.
* stearic. % de ácido esteárico en la muestra.
* oleic. % de ácido oleico en la muestra.
* linoleic. % de ácido linoleico en la muestra.
* linolenic. % de ácido linolénico en la muestra.
* arachidic. % de ácido araquidónico en la muestra.
* eicosenoic. % de ácido eicosenoico en la muestra.

```{r}
library(dslabs)
str(olive)
```



Antes de utilizar cualquier método de clustering vamos a estandarizar los datos:
```{r}
standarized_model <- caret::preProcess(olive, method = c("center", "scale"))
olive_standarized = predict(standarized_model, olive)
```

Vamos a calcular para k=3 grupos como se haría:
```{r}
library(cluster)
k=3
data_olive<-olive_standarized[,3:10]
myclust<-kmeans(data_olive, centers=k)
```

¿Cómo medimos la calidad?

Una forma de medirlo es con el SSE (Sum Square Errors).

* $SSE_{intra}$: Se define para cada grupo. Es la suma de distancias al cuadrado entre los puntos de un cluster y el centroide.
* $SSE_{inter}$: Se define para todos. Es la suma de las distancias al cuadrado entre los centroides de un cluster (ponderado al número de elementos del cluster) y el centroide de los centroides.
* $SSE_{total}$: $SSE_{inter}$+$\sum SSE_{intra}$

Una forma de medir la calidad de un cluster es el ratio:
$$
\frac{SSE_{inter}}{SSE_{total}}
$$
Cuanto más cercano a 1, mejor es la división entre clusters
```{r}
quatily<-myclust$betweenss/myclust$totss
print(quatily)
```

Otra es mediante silhouette:
```{r}
library(cluster)
ss<-silhouette(myclust$cluster, dist(data_olive))    
plot(ss,col=1:k,border=NA)
```


Vamos a calcular para diferente número de clusteres los diferentes valores de kmeans:

```{r}
q<-c()
qsil<-c()
data_olive<-olive_standarized[,3:10]
for (k in 2:20){
    myclust<-kmeans(data_olive,centers=k,nstart=30)
    q[k]<-myclust$betweenss/myclust$totss
    
    ss<-silhouette(myclust$cluster, dist(data_olive))
    qsil[k]<-mean(ss[, "sil_width"])
}
plot(q)
plot(qsil)
```

Vemos que el número óptimo de grupos es 5:
```{r}
k<-5
myclust<-kmeans(olive_standarized[,3:10],centers=k,nstart=30)
```

Podemos ver el silhouette:
```{r}
ss<-silhouette(myclust$cluster, dist(olive_standarized[,3:10]))
plot(ss,col=1:k,border=NA)
```
```{r}
olive$cluster=factor(myclust$cluster)
```

```{r}
library(dplyr)
olive %>% filter(cluster==5) %>%  summary()
```




# Clustering jerárquico

Vamos a pinter un dendrograma para respresentar el clustering jerárquico:

Primero sobre los datos estandarizados calculamos la matriz de distancias:
```{r}
d <- dist(as.matrix(olive_standarized),method = "euclidean")
```

Una vez que tenemos la matriz de distancias pintamos el dendrograma:
```{r}
library(dendextend)
hc <- hclust(d, method="complete")
hcd <- as.dendrogram(hc)
h <- 7.2
plot(color_branches(hcd, h=h),leaflab="none")
abline(h=h,col="red")
```


```{r}
cl<-cutree_1h.dendrogram(hcd,h=h)
olive$cluster = factor(cl)
```

```{r}
olive %>% filter(cluster==3) %>% select(region,area) %>% summary()
```
